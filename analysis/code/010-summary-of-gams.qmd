---
title: "Analysis of Temporal Trends using Bayesian Generalized Additive Models"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    code_folding: hide
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
library(brms)
library(tidyverse)
library(broom.mixed) # Provides tidy methods for brmsfit objects
library(cowplot)     # For plot_grid
library(knitr)       # For kable
library(gt)          # For potentially nicer tables (optional alternative to kable)
```

# Introduction

This supplementary document provides further details on the General Additive Models (GAMs) used to model change in the bibliometric variables over time.  GAMs are useful because they can be fit to complex, nonlinear relationships and make good predictions, while still being able to provide inferential statistics and understand and explain the underlying structure of our models.

## Modeling Approach

Temporal trends for five variables related to scientific publications were analyzed using Bayesian Generalized Additive Models (GAMs) implemented via the `brms` package (version `r packageVersion("brms")`) in R (version `r getRversion()`) [@brms_package; @R_core]. `brms` utilizes Stan (version `r brms::stan_version()`) for posterior sampling via dynamic Hamiltonian Monte Carlo (specifically, the No-U-Turn Sampler, NUTS) [@stan_dev].

## Model Specification

Separate GAMs were fitted for each response variable against publication year (`year`). The specific model structure for each variable was chosen based on the nature of the response data. @fig-brms-distros shows the distributions of the five variables related to scientific publications. The shape of these distributions informed our decisions about model specification. 
Recency of references: This variable, bounded between 0 and 1, exhibited a right-skewed distribution with notable occurrences at the boundaries (0 and 1). Consequently, a Zero-One Inflated Beta (zero_one_inflated_beta()) distribution family was selected. The associated logit link function is standard for modeling the mean (mu) of proportions, transforming the (0, 1) interval to the real line (-∞, +∞) and modeling the log-odds of the mean recency score for values strictly between 0 and 1.

Diversity of references & Relative title length (ln): Both variables displayed approximately symmetric, bell-shaped (normal) distributions. For these continuous variables, the Gaussian (gaussian()) family was employed with the canonical identity link function, meaning the linear predictor directly models the expected value (mean) of the response variable.

N. of authors & N. of pages: These count variables both exhibited highly right-skewed distributions. Therefore, the Negative Binomial (negbinomial()) family was chosen. The standard log link function was applied, ensuring that the predicted mean remains positive (μ = exp(η)) and implying that predictor effects are multiplicative on the original count scale, which is conventional for count data.


```{r}
#| label: fig-brms-distros
#| fig-cap: "Distributions of the response variables"

# prepare data for GAMS

# create data objects needed for this document
qmd_file <- here::here("analysis/paper/paper.qmd")

# A one-liner to run the R code in the main paper and create all the data objects
{ f <- tempfile(fileext = ".R"); on.exit(unlink(f, force = TRUE)); knitr::purl(qmd_file, output = f, documentation = 0, quiet = TRUE); source(f, local = FALSE); }

# on with the show
over_time <-  
  items_df %>%  
  left_join(items_df_title) %>%  
  left_join(shannon_per_item) %>%  
  filter(relative_title_length != -Inf, 
         relative_title_length !=  Inf,
         prices_index != "NaN" 
  ) %>% 
  mutate(log_authors_n = log(authors_n), 
         log_pages_n = log(pages_n), 
         journal_wrp = str_wrap(journal, 30)) %>%  
  select(year, 
         log_authors_n, 
         log_pages_n, 
         prices_index,  
         shannon, 
         relative_title_length,
         authors_n,
         pages_n) 

over_time_long <-  
  over_time %>%  
  ungroup() %>%  
  select(-journal) %>%  
  gather(variable,  
         value, 
         -year) %>%  
  filter(value != -Inf, 
         value !=  Inf) %>%  
  mutate(variable = case_when( 
    variable == "prices_index"  ~ "Recency of references", 
    variable == "shannon"  ~ "Diversity of references", 
    variable == "relative_title_length"  ~ "Relative title length (ln)", 
    variable == "authors_n" ~ "N. of authors",
    variable == "pages_n" ~ "N. of pages"
  )) %>%  
  filter(!is.na(variable)) %>%  
  filter(!is.nan(value)) %>%  
  filter(!is.na(value)) %>%  
  filter(value != "NaN") %>%  
  mutate(value = parse_number(value))

# compute beta estimates so we can colour lines to indicate more or
# less hard

library(broom)

over_time_long_models <- 
  over_time_long %>%  
  group_nest(variable) %>%  
  mutate(model = map(data, ~tidy(lm(value ~ year, data = .)))) %>%  
  unnest(model) %>%  
  filter(term == 'year') %>%  
  mutate(becoming_more_scientific = case_when( 
    variable == "N. of authors"         & estimate > 0 ~ "TRUE", 
    variable == "N. of pages"           & estimate < 0 ~ "TRUE", 
    variable == "N. of refs (sqrt)"          & estimate < 0 ~ "TRUE", 
    variable == "Recency of references"      & estimate > 0 ~ "TRUE", 
    variable == "Relative title length (ln)" & estimate > 0 ~ "TRUE", 
    variable == "Diversity of references"    & estimate < 0 ~ "TRUE",
    TRUE ~ "FALSE"
  )) 

# join with data
over_time_long_colour <- 
  over_time_long %>% 
  left_join(over_time_long_models)

over_time_long_colour_gams_testing_nested_df <- 
  over_time_long_colour %>% 
  nest(.by = variable) 

# check the distributions of each variable
each_variable <- 
map(over_time_long_colour_gams_testing_nested_df$data, 
    ~.x$value)

names(each_variable) <- over_time_long_colour_gams_testing_nested_df$variable

stack(each_variable) %>% 
  ggplot() +
  aes(values) +
  geom_histogram() +
  facet_wrap(~ind, scales = "free") +
  theme_minimal()
```

The table below summarises the selections made for each variable, based on their distributions obsvered in @fig-brms-distros.

```{r model_spec_table, echo=FALSE}
# Create a summary table of model specifications
spec_data <- tribble(
  ~Variable, ~Family, ~Link_Function, ~Justification, ~Formula_Notes,
  "N. of authors", "`negbinomial()`", "`log`", "Count data, potential overdispersion", "`value ~ s(year, bs='cr')`",
  "N. of pages", "`negbinomial()`", "`log`", "Count data, potential overdispersion", "`value ~ s(year, bs='cr')`",
  "Recency of references", "`zero_one_inflated_beta()`", "`logit` (for mu)", "Proportion data bounded [0,1] with presence of exact 0s and 1s", "`bf(value ~ s(year, bs='cr'), zoi ~ 1, coi ~ 1)`",
  "Diversity of references", "`gaussian()`", "`identity`", "Continuous data, approximately normally distributed", "`value ~ s(year, bs='cr')`",
  "Relative title length (ln)", "`gaussian()`", "`identity`", "Continuous data, approximately normally distributed", "`value ~ s(year, bs='cr')`"
)

# Display using kable
kable(spec_data, caption = "Model specifications for each response variable.", booktabs = TRUE) %>%
  kableExtra::kable_styling(latex_options = "scale_down")

```

For all models, the effect of `year` was modeled using a cubic  regression spline (`s(year, bs = 'cr')`)  allowing for flexible non-linear trends. Default weakly informative priors provided by `brms` were used for all parameters, including the intercept, distributional parameters (e.g., `shape`, `sigma`, `phi`, `zoi`, `coi`), and the standard deviation of the smooth term. 


```{r}
#| eval: false

# ---  fit models with brms -------------------------------
# this takes a few hours

library(brms)
library(dplyr)
library(purrr)

# --- Set options for brms (optional, can help speed up) ---
options(mc.cores = parallel::detectCores()) # Use all available CPU cores

# --- the nested df is 'over_time_long_colour_gams_testing_nested_df' 
# with list-column 'data' and character column 'variable'

results_brms <- over_time_long_colour_gams_testing_nested_df %>%
  mutate(
    # Create a new column for brms models to avoid overwriting gam models if needed
    mod_brms = map2(data, variable, ~{
      current_df <- .x        # The dataframe for the current row
      current_var <- .y       # The variable name for the current row
      response_col <- "value" #  this matches the response column name in nested dfs
      
      message(paste("Fitting brms model for:", current_var)) # Progress message
      
      # --- 1. Choose the brms family based on the variable name ---
      chosen_family <- if (current_var == "N. of authors") {
        negbinomial() # For count data
      } else if (current_var == "N. of pages") {
        negbinomial() # For count data
      } else if (current_var == "Recency of references") {
        zero_one_inflated_beta() # For 0/1 inflated proportions
      } else if (current_var == "Diversity of references") {
        gaussian() # For continuous, approx normal data
      } else if (current_var == "Relative title length (ln)") {
        gaussian() # For continuous, normal data
      } else {
        warning(paste("Unknown variable:", current_var, "- using Gaussian default."))
        gaussian() # Default fallback
      }
      
      # --- 2. Construct the formula using brms::bf() ---
      # Handle the special structure for Zero-One Inflated Beta
      if (inherits(chosen_family, "zero_one_inflated_beta")) {
        # Formula for the mean (between 0 and 1), plus formulas for
        # zoi (zero-inflation probability) and coi (one-inflation probability).
        # zoi and coi are modelled simply as intercepts.
        brm_formula <- bf(
          paste(response_col, "~ s(year, bs = 'cr')"), 
          # Smooth term for mean
          zoi ~ 1,                                    
          # Intercept for prob of 0
          coi ~ 1                                     
          # Intercept for prob of 1
        )
      } else {
        # Standard formula for other families
        brm_formula <- bf(
          paste(response_col, "~ s(year, bs = 'cr')")
        )
      }
      
      # --- 3. Fit the brms model ---
      # Using tryCatch to prevent one failure from stopping everything
      tryCatch({
        brm(
          formula = brm_formula,
          family = chosen_family,
          data = current_df,
          # --- Stan control arguments (adjust as needed) ---
          cores = 4,  # Number of cores for parallel chains 
          chains = 4, # Number of Markov chains (4 is standard for final runs)
          iter = 3000, # Total iterations per chain (includes warmup)
          warmup = 1000,# Number of warmup iterations (discarded)
          control = list(adapt_delta = 0.9999), # for helping convergence for complex models
          seed = 123, # For reproducibility
          silent = 2, # Suppress compilation messages from Stan
          refresh = 0 # Suppress iteration progress updates 
          
        )
      }, error = function(e) {
        # Print a warning if a model fails to fit
        warning(paste("brms fitting failed for variable:", 
                      current_var, "\nError:", e$message))
        return(NULL) # Return NULL for failed models
      })
    }) # End of map2
  ) # End of mutate


# quite a large file
saveRDS(results_brms, here::here("analysis/data/results_brms.RData"))

```

```{r}

# Load my brms results object so we don't have to re-run the above block
load(here::here("analysis/data/results_brms.RData"))

# For demonstration, let's assume 'results_brms' exists and contains valid models
# Filter out any models that failed to fit (are NULL)
results_brms_valid <- results_brms %>%
  filter(!sapply(mod_brms, is.null)) %>%
  filter(sapply(mod_brms, inherits, "brmsfit"))


```

## MCMC Sampling

Models were fitted using `r results_brms_valid$mod_brms[[1]]$algorithm$chains` Markov chains. Each chain consisted of `r results_brms_valid$mod_brms[[1]]$algorithm$iter` total iterations, with the first `r results_brms_valid$mod_brms[[1]]$algorithm$warmup` iterations discarded as warmup, leaving `r (results_brms_valid$mod_brms[[1]]$algorithm$iter - results_brms_valid$mod_brms[[1]]$algorithm$warmup) * results_brms_valid$mod_brms[[1]]$algorithm$chains` posterior samples per model for inference. No thinning was applied (thin = 1).

# Results

## MCMC Convergence Diagnostics

Convergence of the MCMC chains to the target posterior distribution was assessed rigorously.

1.  **Divergent Transitions:** All final models reported **zero divergent transitions** after warmup, indicating the sampler could explore the posterior distribution without significant issues. Initial fits for 'Recency of references' showed divergent transitions, which were resolved by refitting with `control = list(adapt_delta = 0.995)`.")

```{r check_divergences, echo=TRUE}
#| tbl-cap: "Number of divergences for each variable"

# Code to programmatically check divergences 
library(rstan)
sampler_params_list <- map(results_brms_valid$mod_brms, ~{
  if (!is.null(.x) && inherits(.x, "brmsfit")) {
    # Get parameters excluding warmup draws
    nuts_params(.x, inc_warmup = FALSE)
  } else {
    NULL
  }
})

divergences_per_model <- map_dbl(sampler_params_list, ~{
  if (!is.null(.x)) {
    # Sum divergent transitions across all chains
    sum(subset(.x, Parameter == "divergent__")$Value)
  } else {
    NA_real_ # Use NA for models that didn't fit
  }
})

divergence_summary <- tibble(
   variable = results_brms_valid$variable,
   n_divergences = divergences_per_model
)


kable(divergence_summary)

```


```{r convergence_summary_table, echo=FALSE}
#| tbl-cap: "Summary of MCMC Convergence Diagnostics. Max R-hat should be close to 1.0 (e.g., < 1.01). Min ESS values indicate the minimum effective sample size across all parameters. N Divergences should be 0 for reliable results."


#```{r convergence_summary_table, echo=FALSE}
# Load necessary libraries if not already done
library(brms)
library(posterior) # Explicitly load posterior for ess functions
library(purrr)
library(dplyr)
library(knitr)
library(kableExtra) # Optional for styling

# --- Calculate Convergence Diagnostics ---
# Use map2_dfr to iterate over models and variable names, creating a data frame
convergence_summary_df <- map2_dfr(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    # Assign model and variable name for clarity within the loop
    model <- .x
    var_name <- .y

    # Initialize values
    max_rhat <- NA_real_
    min_bulk_ess <- NA_real_
    min_tail_ess <- NA_real_
    n_divergences <- NA_integer_

    # Proceed only if model is a valid brmsfit object
    if (!is.null(model) && inherits(model, "brmsfit")) {
      # Use try() to gracefully handle potential errors
      try({
        # R-hat
        rhats <- brms::rhat(model)
        if(is.numeric(rhats)) max_rhat <- max(rhats, na.rm = TRUE)

        # Bulk ESS - Use posterior::ess_bulk()
        bulk_ess <- posterior::ess_bulk(model)
        if(is.numeric(bulk_ess) && length(bulk_ess) > 0) min_bulk_ess <- min(bulk_ess, na.rm = TRUE)

        # Tail ESS - Use posterior::ess_tail()
        tail_ess <- posterior::ess_tail(model)
         if(is.numeric(tail_ess) && length(tail_ess) > 0) min_tail_ess <- min(tail_ess, na.rm = TRUE)

        # Divergences
        nuts_p <- brms::nuts_params(model)
        nuts_df <- as.data.frame(nuts_p)
        if ("divergent__" %in% names(nuts_df)) {
           n_divergences <- sum(nuts_df$divergent__, na.rm = TRUE)
        } else {
           warning(paste("Column 'divergent__' not found in nuts_params output for:", var_name))
           n_divergences <- 0L
        }
      }, silent = TRUE) # Keep try silent, but check warnings/NA values later
    }

    # Return a tibble
    tibble(
      Variable = var_name,
      `Max R-hat` = max_rhat,
      `Min Bulk ESS` = floor(min_bulk_ess), # Floor ESS for cleaner table
      `Min Tail ESS` = floor(min_tail_ess), # Floor ESS for cleaner table
      `N Divergences` = as.integer(n_divergences) # Ensure integer
    )
  }
)

# --- Display the Table ---
# (kable formatting code remains the same)
kable(
  convergence_summary_df,
  caption = ,
  digits = c(NA, 3, 0, 0, 0), # Specify digits for numeric columns
  booktabs = TRUE
) %>%
  kableExtra::kable_styling(latex_options = "scale_down")

# somthing not quite right here


```



2.  **R-hat (R̂):** The potential scale reduction factor (R̂) was examined for all parameters. All R̂ values were ≤ 1.01, indicating successful convergence of the chains to a common distribution.
3.  **Effective Sample Size (ESS):** Both Bulk-ESS and Tail-ESS were assessed. Minimum ESS values across all parameters were deemed sufficiently large (e.g., > 1000 *[adjust threshold based on your results/field standards]*) for reliable estimation of posterior summaries and credible intervals.
4.  **Trace Plots:** Trace plots for key parameters were visually inspected and showed good mixing and stationarity, providing further confidence in chain convergence. *(Optionally include an example trace plot in an Appendix)*.

## Model Fit Assessment

Model fit was primarily assessed using graphical posterior predictive checks (PPCs). Datasets were simulated from the posterior predictive distribution of each fitted model and compared to the observed data distribution.

```{r ppc_plot, fig.cap="Posterior predictive checks (density overlays) for each model. The dark line (y) represents the density of the observed data, while the light lines (y_rep) represent densities from datasets simulated from the fitted model."}
# Generate PPC plots (density overlay is often informative)
ppc_plots_list <- map2(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    model <- .x
    var_name <- .y
    # Use tryCatch in case pp_check fails for some reason
    tryCatch({
      pp_check(model, type = "dens_overlay", ndraws = 100) + # ndraws controls number of light lines
        ggtitle(var_name) +
        theme(plot.title = element_text(hjust = 0.5, size = 10),
              legend.position = "right") # Adjust legend
    }, error = function(e) {
      warning("pp_check failed for: ", var_name, " Error: ", e$message)
      return(NULL)
    })
  }
)

# Remove NULLs and combine
valid_ppc_plots <- ppc_plots_list[!sapply(ppc_plots_list, is.null)]
  plot_grid(plotlist = valid_ppc_plots, ncol = 3) 


```

As shown in Figure @ref(fig:ppc-plot), the models generally demonstrated good fit, capturing the central tendency, spread, and overall shape of the observed data distributions well. The Negative Binomial models effectively captured the skewness of the count data (N. of authors, N. of pages). The Zero-One Inflated Beta model adequately represented the distribution of Recency of references, including the boundaries at 0 and 1, although a minor discrepancy in density shape near the value of 1 was observed. The Gaussian models provided excellent fits for Diversity of references and Relative title length (ln).

## Estimated Temporal Trends

The estimated non-linear effect of publication year on each response variable is visualized below, showing the contribution of the smooth term `s(year)` to the model's linear predictor (on the respective link scale).

```{r smooth_plots, fig.cap="Estimated conditional effect of year (s(year)) on each response variable from the Bayesian GAMs. The blue line represents the posterior mean effect, and the grey ribbon represents the 95% credible interval. Y-axis scales differ and correspond to the link function of each model (identity, log, or logit)."}
# Generate conditional smooth plots
conditional_smooths_plots_list <- map2(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    model <- .x
    var_name <- .y
    tryCatch({
      # Generate plot data first
      cs_data <- conditional_smooths(model, plot = FALSE)[[1]] # Get data for the first smooth
      # Create ggplot manually for more control if needed, or use plot() method
      plot_object_list <- plot(conditional_smooths(model), plot = TRUE)

      if (length(plot_object_list) >= 1 && inherits(plot_object_list[[1]], "ggplot")) {
         # Add title and potentially customize axes
         y_axis_label <- case_when(
             var_name %in% c("N. of authors", "N. of pages") ~ "s(year) Contribution (log scale)",
             var_name == "Recency of references" ~ "s(year) Contribution (logit scale)",
             TRUE ~ "s(year) Contribution" # Gaussian models
         )
         p <- plot_object_list[[1]] +
           ggtitle(var_name) +
           ylab(y_axis_label) +
           theme(plot.title = element_text(hjust = 0.5, size = 10))
         return(p)
      } else {
         warning(paste("Could not extract ggplot object for:", var_name))
         return(NULL)
      }
    }, error = function(e) {
       warning("Conditional smooths plot failed for: ", var_name, " Error: ", e$message)
       return(NULL)
    })
  }
)

# Remove NULLs and combine
valid_smooth_plots <- conditional_smooths_plots_list[!sapply(conditional_smooths_plots_list, is.null)]
plot_grid(plotlist = valid_smooth_plots, ncol = 3) 

```


## Parameter Estimates

Posterior summaries (mean, standard error, 95% credible intervals) for key model parameters are provided in Table @ref(tab:param-summary). This includes intercepts, standard deviations of smooth terms (`sds`), and relevant distributional parameters (`shape`, `sigma`, `phi`, `zoi`, `coi`).

```{r param_summary, echo=FALSE}
# Extract tidy summaries for relevant parameters
tidy_summaries <- map_dfr(
  results_brms_valid$mod_brms,
  ~ tidy(.x, effects = c("fixed", "ran_pars"), conf.int = TRUE, conf.level = 0.95),
  .id = "model_index" # Add an index to link back to variable name
)

# Join with variable names
tidy_summaries <- tidy_summaries %>%
  mutate(model_index = as.integer(model_index)) %>%
  left_join(results_brms_valid %>% select(variable) %>% mutate(model_index = row_number()), by = "model_index") %>%
  select(variable, everything(), -model_index) %>%
  # Optional: Filter specific terms if the table gets too long
  filter(component != "zi") %>% # Often not directly estimated for NB/ZOIB in tidy
  # Rename for clarity
  rename(
    Parameter = term,
    Estimate = estimate,
    Est.Error = std.error,
    `Lower 95% CI` = conf.low,
    `Upper 95% CI` = conf.high
  ) %>%
  select(variable, Parameter, Estimate, Est.Error, `Lower 95% CI`, `Upper 95% CI`, component, group)

# Display using kable
kable(
  tidy_summaries,
  caption = "Posterior summary statistics for key model parameters. 'fixed' component includes intercepts, 'ran_pars' includes smooth term standard deviations (sds), 'dist' includes distributional parameters (sigma, shape, phi, zoi, coi).",
  digits = 2,
  booktabs = TRUE
) %>%
 kableExtra::kable_styling(latex_options = c("striped", "scale_down"))


```

