---
title: "Supplementary Materials on Bayesian Generalized Additive Models used to Analyse Temporal Trends in Job Ads"
author: "Ben Marwick"
date: "`r Sys.Date()`"
bibliography: references.bib
csl:          templates/journal-of-archaeological-science.csl # Insert path for the bib-style
output:
  html_document 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
library(brms)
library(tidyverse)
library(broom.mixed) # Provides tidy methods for brmsfit objects
library(cowplot)     # For plot_grid
library(knitr)       # For kable
library(kableExtra)
library(posterior) # For as_draws_array
library(bayesplot) # For mcmc_trace and nuts_params
```

# Introduction

This supplementary document provides further details on the General Additive Models (GAMs) used to model change in the bibliometric variables over time.  GAMs are useful because they can be fit to complex, nonlinear relationships and make good predictions, while still being able to provide inferential statistics and understand and explain the underlying structure of our models.

## Modeling Approach

Temporal trends for five variables related to scientific publications were analyzed using Bayesian Generalized Additive Models (GAMs) implemented via the `brms` package (version `r packageVersion("brms")`) in R (version `r getRversion()`) [@brms_package; @rcoreteamLanguageEnvironmentStatistical2024]. `brms` utilizes Stan (version `r rstan::stan_version()`) for posterior sampling via dynamic Hamiltonian Monte Carlo (specifically, the No-U-Turn Sampler, NUTS) [@stan_dev].

## Model Specification

Separate GAMs were fitted for each response variable against publication year (`year`). The specific model structure for each variable was chosen based on the nature of the response data. @fig-brms-distros shows the distributions of the five variables related to scientific publications. The shape of these distributions informed our decisions about model specification. 

Recency of references: This variable, bounded between 0 and 1, exhibited a right-skewed distribution with notable occurrences at the boundaries (0 and 1). Consequently, a Zero-One Inflated Beta (zero_one_inflated_beta()) distribution family was selected. The associated logit link function is standard for modeling the mean (mu) of proportions, transforming the (0, 1) interval to the real line (-∞, +∞) and modeling the log-odds of the mean recency score for values strictly between 0 and 1.

Diversity of references & Relative title length (ln): Both variables displayed approximately symmetric, bell-shaped (normal) distributions. For these continuous variables, the Gaussian (gaussian()) family was employed with the canonical identity link function, meaning the linear predictor directly models the expected value (mean) of the response variable.

N. of authors & N. of pages: These count variables both exhibited highly right-skewed distributions. Therefore, the Negative Binomial (negbinomial()) family was chosen. The standard log link function was applied, ensuring that the predicted mean remains positive (μ = exp(η)) and implying that predictor effects are multiplicative on the original count scale, which is conventional for count data.


```{r}
#| label: fig-brms-distros
#| fig-cap: "Distributions of the response variables"

# prepare data for GAMS

# create data objects needed for this document
qmd_file <- here::here("analysis/paper/paper.qmd")

# A one-liner to run the R code in the main paper and create all the data objects
{ f <- tempfile(fileext = ".R"); on.exit(unlink(f, force = TRUE)); knitr::purl(qmd_file, output = f, documentation = 0, quiet = TRUE); source(f, local = FALSE); }

# on with the show
over_time <-  
  items_df %>%  
  left_join(items_df_title) %>%  
  left_join(shannon_per_item) %>%  
  filter(relative_title_length != -Inf, 
         relative_title_length !=  Inf,
         prices_index != "NaN" 
  ) %>% 
  mutate(log_authors_n = log(authors_n), 
         log_pages_n = log(pages_n), 
         journal_wrp = str_wrap(journal, 30)) %>%  
  select(year, 
         log_authors_n, 
         log_pages_n, 
         prices_index,  
         shannon, 
         relative_title_length,
         authors_n,
         pages_n) 

over_time_long <-  
  over_time %>%  
  ungroup() %>%  
  select(-journal) %>%  
  gather(variable,  
         value, 
         -year) %>%  
  filter(value != -Inf, 
         value !=  Inf) %>%  
  mutate(variable = case_when( 
    variable == "prices_index"  ~ "Recency of references", 
    variable == "shannon"  ~ "Diversity of references", 
    variable == "relative_title_length"  ~ "Relative title length (ln)", 
    variable == "authors_n" ~ "N. of authors",
    variable == "pages_n" ~ "N. of pages"
  )) %>%  
  filter(!is.na(variable)) %>%  
  filter(!is.nan(value)) %>%  
  filter(!is.na(value)) %>%  
  filter(value != "NaN") %>%  
  mutate(value = parse_number(value))

# compute beta estimates so we can colour lines to indicate more or
# less hard

library(broom)

over_time_long_models <- 
  over_time_long %>%  
  group_nest(variable) %>%  
  mutate(model = map(data, ~tidy(lm(value ~ year, data = .)))) %>%  
  unnest(model) %>%  
  filter(term == 'year') %>%  
  mutate(becoming_more_scientific = case_when( 
    variable == "N. of authors"         & estimate > 0 ~ "TRUE", 
    variable == "N. of pages"           & estimate < 0 ~ "TRUE", 
    variable == "N. of refs (sqrt)"          & estimate < 0 ~ "TRUE", 
    variable == "Recency of references"      & estimate > 0 ~ "TRUE", 
    variable == "Relative title length (ln)" & estimate > 0 ~ "TRUE", 
    variable == "Diversity of references"    & estimate < 0 ~ "TRUE",
    TRUE ~ "FALSE"
  )) 

# join with data
over_time_long_colour <- 
  over_time_long %>% 
  left_join(over_time_long_models)

over_time_long_colour_gams_testing_nested_df <- 
  over_time_long_colour %>% 
  nest(.by = variable) 

# check the distributions of each variable
each_variable <- 
map(over_time_long_colour_gams_testing_nested_df$data, 
    ~.x$value)

names(each_variable) <- over_time_long_colour_gams_testing_nested_df$variable

stack(each_variable) %>% 
  ggplot() +
  aes(values) +
  geom_histogram() +
  facet_wrap(~ind, scales = "free") +
  theme_minimal()
```

The table below summarises the selections made for each variable, based on their distributions obsvered in @fig-brms-distros.

```{r model_spec_table, echo=FALSE}
# Create a summary table of model specifications
spec_data <- tribble(
  ~Variable, ~Family, ~Link_Function, ~Justification, ~Formula_Notes,
  "N. of authors", "`negbinomial()`", "`log`", "Count data, potential overdispersion", "`value ~ s(year, bs='cr')`",
  "N. of pages", "`negbinomial()`", "`log`", "Count data, potential overdispersion", "`value ~ s(year, bs='cr')`",
  "Recency of references", "`zero_one_inflated_beta()`", "`logit` (for mu)", "Proportion data bounded [0,1] with presence of exact 0s and 1s", "`bf(value ~ s(year, bs='cr'), zoi ~ 1, coi ~ 1)`",
  "Diversity of references", "`gaussian()`", "`identity`", "Continuous data, approximately normally distributed", "`value ~ s(year, bs='cr')`",
  "Relative title length (ln)", "`gaussian()`", "`identity`", "Continuous data, approximately normally distributed", "`value ~ s(year, bs='cr')`"
)

# Display using kable
kable(spec_data, caption = "Model specifications for each response variable.", booktabs = TRUE) %>%
  kableExtra::kable_styling(latex_options = "scale_down")

```

For all models, the effect of `year` was modeled using a cubic  regression spline (`s(year, bs = 'cr')`)  allowing for flexible non-linear trends. Default weakly informative priors provided by `brms` were used for all parameters, including the intercept, distributional parameters (e.g., `shape`, `sigma`, `phi`, `zoi`, `coi`), and the standard deviation of the smooth term. 


```{r}
#| eval: false

# ---  fit models with brms -------------------------------
# this takes a few hours

# --- Set options for brms 
options(mc.cores = parallel::detectCores()) # Use all available CPU cores

# --- the nested df is 'over_time_long_colour_gams_testing_nested_df' 
# with list-column 'data' and character column 'variable'

# --- Define the Prior ---
# Apply a regularizing prior to the standard deviation of all smooth terms
# Exponential(1) encourages smaller standard deviations (less wiggly smooths)
sds_prior <- prior(exponential(1), class = "sds")

# --- Run the models with the added prior ---
results_brms <- over_time_long_colour_gams_testing_nested_df %>%
  mutate(
    # Create a new column for brms models
    mod_brms = map2(data, variable, ~{
      current_df <- .x        # The dataframe for the current row
      current_var <- .y       # The variable name for the current row
      response_col <- "value" # Ensure this matches

      message(paste("Fitting brms model for:", current_var))

      # --- 1. Choose the brms family ---
      chosen_family <- if (current_var == "N. of authors") {
        negbinomial()
      } else if (current_var == "N. of pages") {
        negbinomial()
      } else if (current_var == "Recency of references") {
        zero_one_inflated_beta()
      } else if (current_var == "Diversity of references") {
        gaussian()
      } else if (current_var == "Relative title length (ln)") {
        gaussian()
      } else {
        warning(paste("Unknown variable:", current_var, "- using Gaussian default."))
        gaussian()
      }

      # --- 2. Construct the formula ---
      if (inherits(chosen_family, "zero_one_inflated_beta")) {
        brm_formula <- bf(
          paste(response_col, "~ s(year, bs = 'cr')"),
          zoi ~ 1,
          coi ~ 1
        )
      } else {
        brm_formula <- bf(
          paste(response_col, "~ s(year, bs = 'cr')")
        )
      }

      # --- 3. Fit the brms model with the prior ---
      tryCatch({
        brm(
          formula = brm_formula,
          family = chosen_family,
          data = current_df,
          prior = sds_prior, 
          cores =    4,
          chains =   4,
          iter =     50000,
          warmup =   5000,
          control =  list(adapt_delta = 0.99999), 
          seed =     123,
          silent =   2,
          refresh =  0
        )
      }, error = function(e) {
        warning(paste("brms fitting failed for variable:",
                      current_var, "\nError:", e$message))
        return(NULL)
      })
    }) # End of map2
  ) # End of mutate


# quite a large file
saveRDS(results_brms, here::here("analysis/data/results_brms.RData"))

```

```{r}

# Load my brms results object so we don't have to re-run the above block
results_brms <- readRDS(here::here("analysis/data/results_brms.RData"))

# Filter out any models that failed to fit (are NULL)
results_brms_valid <- results_brms %>%
  filter(!sapply(mod_brms, is.null)) %>%
  filter(sapply(mod_brms, inherits, "brmsfit"))

pretty_num <- function(x) format(x, big.mark = ",", scientific = FALSE)

n_chains <- pretty_num(brms::nchains(results_brms_valid$mod_brms[[1]]))
n_total_iter <- pretty_num(results_brms_valid$mod_brms[[1]]$fit@sim$iter)
n_warmup <- pretty_num(results_brms_valid$mod_brms[[1]]$fit@sim$warmup)
n_pos_samples <- 
  pretty_num(
  (results_brms_valid$mod_brms[[1]]$fit@sim$iter - results_brms_valid$mod_brms[[1]]$fit@sim$warmup) * brms::nchains(results_brms_valid$mod_brms[[1]])
  )


```

## MCMC Sampling

Models were fitted using `r n_chains` Markov chains. Each chain consisted of `r n_total_iter` total iterations, with the first `r n_warmup` iterations discarded as warmup, leaving `r n_pos_samples` posterior samples per model for inference. No thinning was applied (thin = 1).

# Results

## MCMC Convergence Diagnostics

Convergence of the MCMC chains to the target posterior distribution was assessed rigorously (@tbl-convergence-summary-table).


```{r , echo=FALSE}
#| label: tbl-convergence-summary-table
#| tbl-cap: "Summary of MCMC Convergence Diagnostics. Max R-hat should be close to 1.0 (e.g., < 1.01). Min ESS values indicate the minimum effective sample size across all parameters. N Divergences should be 0 for reliable results."

# Function to extract diagnostics from a single model
get_diagnostics <- function(m) {
  ms <- summary(m)
  
  tibble(
    chains = brms::nchains(m),
    rhat_max = max(brms::rhat(m)), 
    bulk_ess_min = min(ms$spec_pars$Bulk_ESS, 
                       na.rm = TRUE),
    tail_ess_min = min(ms$spec_pars$Tail_ESS, 
                       na.rm = TRUE),
    num_divergent = rstan::get_num_divergent(m$fit)
  )
}

# Apply to all models
diagnostics_table <- results_brms_valid %>%
  mutate(diagnostics = map(mod_brms, get_diagnostics)) %>%
  unnest(diagnostics) %>% 
  select(variable, 
         chains,
         rhat_max,
         bulk_ess_min,
         tail_ess_min,
         num_divergent)

# View the table

library(knitr)
library(kableExtra)

kable(
  diagnostics_table,
  digits = c(NA, 0, 3, 0, 0, 0), # Chains/ESS/Div as integers, Rhat to 3 decimals
  booktabs = TRUE,
  linesep = ""
) %>%
  kableExtra::kable_styling(latex_options = "scale_down", full_width = FALSE) %>%
  kableExtra::footnote(general = "Ideal values: Max R-hat ≈ 1.0, Min ESS > 1000, N Divergences = 0.", general_title = "Note:", footnote_as_chunk = TRUE)
```

1.  **Divergent Transitions:** All final models reported less than ten **divergent transitions** after warmup, indicating the sampler could explore the posterior distribution without significant issues. Initial fits for 'Recency of references' showed divergent transitions, which were managed by refitting with `control = list(adapt_delta = 0.9999)`.")
2.  **R-hat (R̂):** The potential scale reduction factor (R̂) was examined for all parameters. All R̂ values were ≤ 1.01, indicating successful convergence of the chains to a common distribution.
3.  **Effective Sample Size (ESS):** Both Bulk-ESS and Tail-ESS were assessed. Minimum ESS values across all parameters were deemed sufficiently large (> 1000) for reliable estimation of posterior summaries and credible intervals.
4.  **Trace Plots:** Trace plots for key parameters were visually inspected and showed good mixing and stationarity, providing further confidence in chain convergence (@fig-trace-plots-mcmc-trace).


```{r fig.height=3 * length(results_brms_valid$mod_brms)}
#| label: fig-trace-plots-mcmc-trace
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-cap: "Trace plots for key parameters (Intercept and smooth term standard deviation) with divergence markers for each model. Well-mixing chains should resemble stationary 'fuzzy caterpillars' with chains overlapping."
 
#---------------------------------------------------------------------
# Generate Trace Plots using mcmc_trace for each model
#---------------------------------------------------------------------
trace_plots_list <- map2(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    # Assign model and variable name for clarity
    model <- .x
    var_name <- .y

    # Initialize plot object as NULL
    p <- NULL

    # Proceed only if model is a valid brmsfit object
    if (!is.null(model) && inherits(model, "brmsfit")) {
      try({
        # --- Define parameters to plot ---
        # Use the names confirmed to work: b_Intercept and sds_syear_1
        # Check if these parameters actually exist in the current model
        all_model_pars <- parnames(model)
        pars_to_plot <- character(0) # Start with empty vector

        if ("b_Intercept" %in% all_model_pars) {
          pars_to_plot <- c(pars_to_plot, "b_Intercept")
        } else {
           warning(paste("'b_Intercept' not found for model:", var_name))
        }

        if ("sds_syear_1" %in% all_model_pars) {
           pars_to_plot <- c(pars_to_plot, "sds_syear_1")
        } else {
           # Handle cases where the smooth sds name might differ slightly
           # Example: find any parameter starting with sds_s
           sds_par_alt <- grep("^sds_s", all_model_pars, value = TRUE)
           if(length(sds_par_alt) > 0) {
               pars_to_plot <- c(pars_to_plot, sds_par_alt[1])
               warning(paste("Using alternative sds name:", sds_par_alt[1], "for model:", var_name))
           } else {
               warning(paste("'sds_syear_1' (or similar) not found for model:", var_name))
           }
        }
        # --- End of parameter identification ---

        # Proceed only if we found parameters to plot
        if(length(pars_to_plot) > 0) {
            # 1) Extract draws as an array
            post_array <- posterior::as_draws_array(model)

            # 2) Extract NUTS diagnostics (divergences)
            np <- brms::nuts_params(model)

            # 3) Generate trace plot with divergence ticks
            p <- bayesplot::mcmc_trace(
                    post_array,
                    pars = pars_to_plot, # Use the identified parameters
                    np = np, # Provide NUTS parameters for divergence markers
                    facet_args = list(ncol = 1, strip.position = "left") # Arrange vertically
                  ) +
                 ggtitle(var_name) + # Add variable name as title
                 theme(
                    plot.title = element_text(hjust = 0.5, size = 11), # Center title
                    legend.position = "none" # Often hide legend
                    )
        } else {
            message(paste("No parameters selected for plotting trace for model:", var_name))
        }

      }, silent = TRUE) # Use try silently, check for NULL later
    } else {
        warning(paste("Invalid or NULL model object encountered for variable:", var_name))
    }

    # Return the ggplot object (or NULL if failed or no pars found)
    return(p)
  }
)

# Combine and Display Plots
#---------------------------------------------------------------------
# Remove any NULL elements from the list
valid_trace_plots <- trace_plots_list[!sapply(trace_plots_list, is.null)]

# Check if there are any valid plots left
if (length(valid_trace_plots) > 0) {
  # Combine the valid plots into a grid
  # Adjust ncol and fig.height chunk option as needed
  combined_trace_plots <- plot_grid(
                              plotlist = valid_trace_plots,
                              ncol = 2 # Arrange in 2 columns
                              )

  # Display the combined plot
  combined_trace_plots
} else {
  message("No valid trace plots were generated.")
}
#---------------------------------------------------------------------
```


## Model Fit Assessment

Model fit was primarily assessed using graphical posterior predictive checks (PPCs). Datasets were simulated from the posterior predictive distribution of each fitted model and compared to the observed data distribution (@fig-ppc-plot).

```{r}
#| label: fig-ppc-plot
#| fig-cap: "Posterior predictive checks (density overlays) for each model. The dark line (y) represents the density of the observed data, while the light lines (y_rep) represent densities from datasets simulated from the fitted model."
#| echo: true  
#| warning: false  
#| message: false  
# Generate PPC plots (density overlay is often informative)
ppc_plots_list <- map2(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    model <- .x
    var_name <- .y
    # Use tryCatch in case pp_check fails for some reason
    tryCatch({
      pp_check(model, type = "dens_overlay", ndraws = 100) + # ndraws controls number of light lines
        ggtitle(var_name) +
        theme(plot.title = element_text(hjust = 0.5, size = 10),
              legend.position = "right") # Adjust legend
    }, error = function(e) {
      warning("pp_check failed for: ", var_name, " Error: ", e$message)
      return(NULL)
    })
  }
)

# Remove NULLs and combine
valid_ppc_plots <- ppc_plots_list[!sapply(ppc_plots_list, is.null)]
plot_grid(plotlist = valid_ppc_plots, ncol = 3) 


```

As shown in @fig-ppc-plot, the models generally demonstrated good fit, capturing the central tendency, spread, and overall shape of the observed data distributions well. The Negative Binomial models effectively captured the skewness of the count data (N. of authors, N. of pages). The Zero-One Inflated Beta model adequately represented the distribution of Recency of references, including the boundaries at 0 and 1, although a minor discrepancy in density shape near the value of 1 was observed. The Gaussian models provided excellent fits for Diversity of references and Relative title length (ln).

## Parameter Estimates

Posterior summaries (mean, standard error, 95% credible intervals) for key model parameters are provided in @tbl-param-summary. This includes intercepts, standard deviations of smooth terms (`sds`), and relevant distributional parameters (`shape`, `sigma`, `phi`, `zoi`, `coi`).


```{r}
#| label: tbl-param-summary
#| message: false
#| warning: false

#---------------------------------------------------------------------
# Extract posterior summaries using brms::posterior_summary
#---------------------------------------------------------------------
# Apply posterior_summary to each model, keep results in a list
posterior_summaries_list <- map(
  results_brms_valid$mod_brms,
  ~ posterior_summary(.x, probs = c(0.025, 0.975)) # Get mean, sd, and 95% CrI
)

# --- Define the key parameters to keep in the final table ---
# Verify these names against your actual model output if needed
params_to_keep <- c(
    "b_Intercept",      # Intercept
    "sigma",            # Residual SD for Gaussian
    "shape",            # Shape parameter for Negative Binomial
    "phi",              # Precision for Beta/ZOIB
    "zoi",              # Zero-inflation probability for ZOIB
    "coi",              # One-inflation probability for ZOIB
    "sds_syear_1"       # SD for the smooth term s(year)
)

#---------------------------------------------------------------------
# Process the list into a final data frame
#---------------------------------------------------------------------
tidy_summaries_final <- map2_dfr(
  posterior_summaries_list,
  results_brms_valid$variable,
  ~{
    # Convert matrix output to tibble
    summary_df <- as_tibble(.x, rownames = "Parameter")

    # Filter for desired parameters
    summary_filtered <- summary_df %>%
      filter(Parameter %in% params_to_keep)

    # Add variable name and parameter type classification
    summary_filtered %>%
      mutate(
        variable = .y, # Add variable name
        `Parameter Type` = case_when(
            Parameter == "b_Intercept" ~ "Intercept",
            Parameter %in% c("sigma", "shape", "phi", "zoi", "coi") ~ "Distributional",
            startsWith(Parameter, "sds_") ~ "Smooth SD",
            TRUE ~ "Other"
        ),
        # Rename parameters for better readability
        Parameter = case_when(
            Parameter == "b_Intercept" ~ "(Intercept)",
            Parameter == "sds_syear_1" ~ "sds(s(year))",
            TRUE ~ Parameter
        )
      )
  }
)

# --- Select and order columns for the final table ---
tidy_summaries_final <- tidy_summaries_final %>%
    rename(
        Estimate = Estimate,
        Est.Error = Est.Error,
        `Lower 95% CI` = Q2.5, # Rename quantile column
        `Upper 95% CI` = Q97.5 # Rename quantile column
    ) %>%
    select(
        variable, Parameter, `Parameter Type`,
        Estimate, Est.Error,
        `Lower 95% CI`, `Upper 95% CI`
    ) %>%
    arrange(variable, `Parameter Type`, Parameter)

#---------------------------------------------------------------------
# Create and Display the Formatted Table
#---------------------------------------------------------------------
# Check if the final tibble is empty
if(nrow(tidy_summaries_final) == 0) {
    warning("Final parameter summary table is empty. Check parameter names in 'params_to_keep'.")
} else {
    kable(
      tidy_summaries_final,
      caption = "Posterior summary statistics for key model parameters.",
      digits = 2,
      booktabs = TRUE,
      linesep = ""
    ) %>%
     kableExtra::kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        latex_options = c("striped", "scale_down"),
        full_width = FALSE
        ) %>%
     kableExtra::column_spec(1, bold = TRUE) # Make variable name bold
}
```

# R-squared Equivalent for Model Fit

Standard R-squared isn't directly applicable to many GAM families (especially non-Gaussian ones). The recommended metric for brms models is the Bayesian R-squared. Calculated using brms::bayes_R2(model), this metric represents the variance of the predicted values divided by the sum of the variance of the predicted values and the expected variance of the residuals. It estimates the proportion of outcome variance that can be explained by the model's predictors. The results are shown below in @tbl-r-sq. 

```{r}
#| label: tbl-r-sq
#| tbl-cap: "Bayesian R-squared estimates for each model."
#| message: false
#| warning: false


# Calculate Bayesian R-squared for each model
# Use map to apply to each model, map_dfr to combine
bayes_r2_summary <- map2_dfr(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    model <- .x
    var_name <- .y
    r2_est <- NA_real_
    r2_lower <- NA_real_
    r2_upper <- NA_real_

           # Calculate Bayesian R2 (returns matrix with Estimate, Est.Error, Q2.5, Q97.5)
           r2_obj <- brms::bayes_R2(model, ndraws = 10000)
           # Extract the point estimate and CI bounds
           r2_est <- r2_obj[1, "Estimate"]
           r2_lower <- r2_obj[1, "Q2.5"]
           r2_upper <- r2_obj[1, "Q97.5"]
           
    tibble(
        Variable = var_name,
        `Bayes R2` = r2_est,
        `Lower 95% CI` = r2_lower,
        `Upper 95% CI` = r2_upper
        )
  }
)

kable(bayes_r2_summary, 
      digits = 2, 
      booktabs = TRUE) %>%
kable_styling(latex_options = "scale_down")
```


## Estimated Temporal Trends

The estimated non-linear effect of publication year on each response variable is visualized below (@fig-smooth-plots), showing the contribution of the smooth term `s(year)` to the model's linear predictor (on the respective link scale). This allows us to visually assess the significance of the model. 

Recency of references: There is strong evidence for a significant temporal trend. The 95% CI falls convincingly below zero from approximately the early 1990s onwards, indicating a significant decrease in the contribution of year to recency (on the logit scale) during the later decades of the study period. Evidence for an effect differing from zero in the earliest years is less conclusive as the CI approaches the zero line.

Diversity of references: The effect of year on reference diversity shows strong evidence of being non-zero across most of the timeframe. The 95% CI is clearly separated from zero, initially below and later rising significantly above it, particularly after 2010, indicating substantial temporal variation not attributable to chance.

Relative title length (ln): The evidence suggests a significant effect of year primarily concentrated around 1990, where the 95% CI lies entirely below the zero line. During the initial years and from roughly 2000 onwards, the CI overlaps zero, indicating that the evidence for an effect of year being different from zero is weak or uncertain in these periods.

N. of authors: Strong evidence supports a significant positive effect of year on the number of authors (on the log scale). The 95% CI is consistently above the zero line from the late 1980s onwards, demonstrating a significant increasing trend over the majority of the period examined.

N. of pages: The temporal effect on the number of pages appears significant during specific periods. The 95% CI is clearly above zero during the peak in the 1990s, providing strong evidence for an increased effect during that time. However, the CI overlaps zero during the trough in the mid-2000s and potentially near the end of the period, suggesting uncertainty about whether the effect differs significantly from zero during these phases. There is also evidence for a negative effect relative to the intercept in the earliest years.


```{r}
#| label: fig-smooth-plots
#| fig-cap: "Estimated conditional effect of year (s(year)) on each response variable from the Bayesian GAMs. The blue line represents the posterior mean effect, and the grey ribbon represents the 95% credible interval. Y-axis scales differ and correspond to the link function of each model (identity, log, or logit). The red line is y=0 to visually assess the significance of the  (i.e., whether the credible interval excludes zero)."

# Generate conditional smooth plots within map2
conditional_smooths_plots_list <- map2(
  results_brms_valid$mod_brms,
  results_brms_valid$variable,
  ~{
    # Assign model and variable name
    model <- .x
    var_name <- .y

    # Generate the plot object list directly
    plot_object_list <- plot(conditional_smooths(model), plot = FALSE)

    # Define the y-axis label based on variable name (link function)
    y_axis_label <- case_when(
        var_name %in% c("N. of authors", "N. of pages") ~ "s(year) Contribution (log scale)",
        var_name == "Recency of references" ~ "s(year) Contribution (logit scale)",
        TRUE ~ "s(year) Contribution" # Default for Gaussian models
    )

    # Extract the first plot, add title, label, and theme
    plot_object_list[[1]] + 
      geom_hline(yintercept = 0, 
                 color = "red", 
                 linetype = "dashed", 
                 linewidth = 0.8) +
      ggtitle(var_name) +
      ylab(y_axis_label) +
      theme_minimal() +
      theme(plot.title = element_text(hjust = 0.5, size = 10)) # Center title
  }
)

# plot a panel of all the plots
plot_grid(plotlist = conditional_smooths_plots_list, ncol = 3)
```


```{r}
# plot for manuscript
# Generate conditional smooth plots within pmap

input_list <-   list(
    model = results_brms_valid$mod_brms,
    var_name = results_brms_valid$variable,
    raw_data = results_brms_valid$data,
    r_sq = bayes_r2_summary$`Bayes R2`
  )

conditional_smooths_plots_fn <- function(model, var_name, raw_data, r_sq){
    
# Generate the plot object list 
     smooth_data <- conditional_smooths(model, plot = FALSE)[[1]]
     intercept_est <- fixef(model)["Intercept", "Estimate"]
     
# --- Determine inverse link function and response label ---
     
        inv_link <- identity # Default for Gaussian
        response_label <- var_name # Default label

        if (var_name %in% c("N. of authors", "N. of pages")) {
          inv_link <- exp # Inverse of log link
          response_label <- paste("Estimated", var_name)
        } else if (var_name == "Recency of references") {
          inv_link <- plogis # Inverse of logit link
          response_label <- "Estimated Recency of references (Price Index)"
        } else if (var_name == "Diversity of references") {
           response_label <- "Estimated Diversity of references"
        } else if (var_name == "Relative title length (ln)") {
           response_label <- "Estimated Rel. Title Length (ln)"
        }
        
# --- Add intercept and apply inverse link function so we can plot on the scale of the raw data
        smooth_data <- 
          smooth_data %>%
          mutate(
            # Calculate estimates on response scale
            response_est = inv_link(intercept_est + estimate__),
            response_lower = inv_link(intercept_est + lower__),
            response_upper = inv_link(intercept_est + upper__)
          ) 
        
    ggplot(smooth_data, 
           aes(x = year, 
               y = response_est)) +
            geom_point(data = raw_data %>% 
                         mutate(colour_hex = case_when(
                           becoming_more_scientific == "TRUE" ~ "#1b9e77",
                           .default = "#d95f02"
                         )),
                 aes(x = year,
                     y = value,
                     colour = colour_hex),
                 alpha = 0.7) +
      scale_color_identity() +
      # Add the line 
      geom_line(color = "#7570b3", 
                linewidth = 2) +
      geom_text(data = raw_data  %>% 
                  mutate(`Bayes R2` = r_sq) %>% 
                  mutate(max_value = max(value)) %>% 
                  distinct(`Bayes R2`, 
                           max_value),
           aes(
           x = 1980, 
           y = max_value, 
           label = paste("Pseudo R² = ", 
                         signif(`Bayes R2`, 
                                digits = 3))),
           hjust = 0, 
           vjust = 1.5,
           size = 2) +
    theme_bw(base_size = 4) +
      labs( 
           y = response_label, 
           x = "")
}

conditional_smooths_plots_list <- 
  pmap(input_list,
       conditional_smooths_plots_fn)
```

Here is the figure that appears in the main manuscript:

```{r}
#| label: fig-smooth-plots-paper
#| fig-cap: "Distribution of article characteristics for archaeology articles over time and estimated conditional effect of year (purple lines). Data points represent individual articles. The colour of the points indicates if the overall temporal trend is toward archaeology become a softer (orange) or harder (green) field. Bayesian Generalized Additive Models were computed to fit the lines summarising the relationships between the variables and the time series. For recency of references, a Zero-One Inflated Beta distribution family was used with a logit link function. For diversity of references and relative title length (ln), the Gaussian family was used with the canonical identity link function. For the number of authors pages the Negative Binomial family was used with a standard log link function. Further details about the model specifications and diagnostics are presented in the supplementary materials." 

plot_grid(plotlist = conditional_smooths_plots_list, 
          ncol = 3,
          align = "hv",
          axis ="tblr")

ggsave(here::here("analysis/figures/fig-smooth-plots-paper.png"),
      dpi = 1000,
      bg = "white",
      h = 3,
      w = 5)

```


